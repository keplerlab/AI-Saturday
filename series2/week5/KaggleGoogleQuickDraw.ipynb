{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Competition for Google Quickdraw https://www.kaggle.com/c/quickdraw-doodle-recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Quick, Draw!\" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories.\n",
    "\n",
    "Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You’ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution.\n",
    "\n",
    "Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put these at the top of every notebook, to get automatic reloading and inline plotting\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import ast\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import the libraries we need. We'll learn about what each does during the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains all the main external libs we'll use\n",
    "from fastai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.transforms import *\n",
    "from fastai.conv_learner import *\n",
    "from fastai.model import *\n",
    "from fastai.dataset import *\n",
    "from fastai.sgdr import *\n",
    "from fastai.plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PATH` is the path to your data - if you use the recommended setup approaches from the lesson, you won't need to change this. `sz` is the size that the images will be resized to in order to ensure that the training runs quickly. We'll be talking about this parameter a lot during the course. Leave it at `224` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data2/\"\n",
    "sz=224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important that you have a working NVidia GPU set up. The programming framework used to behind the scenes to work with NVidia GPUs is called CUDA. Therefore, you need to ensure the following line returns `True` before you proceed. If you have problems with this, please check the FAQ and ask for help on [the forums](http://forums.fast.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, NVidia provides special accelerated functions for deep learning in a package called CuDNN. Although not strictly necessary, it will improve training performance significantly, and is included by default in all supported fastai configurations. Therefore, if the following does not return `True`, you may want to look into why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('data/dogscats/models', exist_ok=True)\n",
    "\n",
    "# !ln -s /datasets/fast.ai/dogscats/train {PATH}\n",
    "# !ln -s /datasets/fast.ai/dogscats/test {PATH}\n",
    "# !ln -s /datasets/fast.ai/dogscats/valid {PATH}\n",
    "\n",
    "# os.makedirs('/cache/tmp', exist_ok=True)\n",
    "# !ln -fs /cache/tmp {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('/cache/tmp', exist_ok=True)\n",
    "# !ln -fs /cache/tmp {PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look at Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Need to look at csv files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide Model between training and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_PATH = 'data2/simplifiedTrainImages2k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMGS=\"simplifiedTrainImages2k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_PATH = 'data/quickdraw/tmp/'\n",
    "MODEL_PATH = 'data/quickdraw/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDirs=[]\n",
    "\n",
    "for idx,iDir in enumerate(listdir(TRAIN_IMG_PATH)):\n",
    "    joinedEntry = os.path.join(TRAIN_IMG_PATH,iDir)\n",
    "    if os.path.isdir(joinedEntry):\n",
    "        #print(joinedEntry)\n",
    "        allDirs.append(iDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(allDirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_file = \"data2/train2kLabels.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data2/simplifiedTrainImages2k/\n"
     ]
    }
   ],
   "source": [
    "train_dir=TRAIN_IMG_PATH\n",
    "print(train_dir)\n",
    "search_terms = allDirs\n",
    "#print( \"Search term list: '%s'\" % search_terms )\n",
    "\n",
    "f= open(labels_file,\"w+\")\n",
    "f.write(\"file,label\\n\")\n",
    "for search_term_dir in search_terms:\n",
    "    #print( \"search term dir: '%s'\" % search_term_dir )\n",
    "    path = os.path.join( train_dir, search_term_dir )\n",
    "    files = os.listdir( path )\n",
    "    for file in files[:200]: #list[:10]\n",
    "        if file.endswith(\".png\"):\n",
    "            #print(search_term_dir + \"/\" + file + \" , \" + search_term_dir)\n",
    "            f.write(search_term_dir + \"/\" + file + \",\" + search_term_dir + \"\\n\")\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first model: quick start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a <b>pre-trained</b> model, that is, a model created by some one else to solve a different problem. Instead of building a model from scratch to solve a similar problem, we'll use a model trained on ImageNet (1.2 million images and 1000 classes) as a starting point. The model is a Convolutional Neural Network (CNN), a type of Neural Network that builds state-of-the-art models for computer vision. We'll be learning all about CNNs during this course.\n",
    "\n",
    "We will be using the <b>resnet34</b> model. resnet34 is a version of the model that won the 2015 ImageNet competition. Here is more info on [resnet models](https://github.com/KaimingHe/deep-residual-networks). We'll be studying them in depth later, but for now we'll focus on using them effectively.\n",
    "\n",
    "Here's how to train and evalulate a *dogs vs cats* model in 3 lines of code, and under 20 seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below if you need to reset your precomputed activations\n",
    "# shutil.rmtree(f'{PATH}tmp', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *learning rate* determines how quickly or how slowly you want to update the *weights* (or *parameters*). Learning rate is one of the most difficult parameters to set, because it significantly affects model performance.\n",
    "\n",
    "The method `learn.lr_find()` helps you find an optimal learning rate. It uses the technique developed in the 2015 paper [Cyclical Learning Rates for Training Neural Networks](http://arxiv.org/abs/1506.01186), where we simply keep increasing the learning rate from a very small value, until the loss stops decreasing. We can plot the learning rate across batches to see what this looks like.\n",
    "\n",
    "We first create a new learner, since we want to know how to set the learning rate for a new (untrained) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"allTestImagesSimplified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(list(open(labels_file)))-1\n",
    "val_idxs = get_cv_idxs(n,val_pct=0.2)\n",
    "arch=resnet34\n",
    "def get_data(sz,bs=64):\n",
    "    tfms = tfms_from_model(resnet34, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\n",
    "    return ImageClassifierData.from_csv(PATH,TRAIN_IMGS,\n",
    "                                        labels_file,\n",
    "                                        tfms=tfms,\n",
    "                                        bs=bs,\n",
    "                                        suffix='',\n",
    "                                        val_idxs=val_idxs,\n",
    "                                        test_name=TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data(sz,bs=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner.pretrained(arch, data, tmp_name=TMP_PATH, models_name=MODEL_PATH,precompute=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#learn = ConvLearner.pretrained(arch, data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lrf=learn.lr_find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `learn` object contains an attribute `sched` that contains our learning rate scheduler, and has some convenient plotting functionality including this one:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the previous plot *iteration* is one iteration (or *minibatch*) of SGD. In one epoch there are \n",
    "(num_train_samples/batch_size) iterations of SGD.\n",
    "\n",
    "We can see the plot of loss versus learning rate to see where our loss stops decreasing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is still clearly improving at lr=10-1 (0.1), so that's what we use. Note that the optimal learning rate can change as we train the model, so you may want to re-run this function from time to time."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "learn.fit(0.1, 4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#learn.fit(0.01, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try training for more epochs, you'll notice that we start to *overfit*, which means that our model is learning to recognize the specific images in the training set, rather than generalizing such that we also get good results on the validation set. One way to fix this is to effectively create more data, through *data augmentation*. This refers to randomly changing the images in ways that shouldn't impact their interpretation, such as horizontal flipping, zooming, and rotating.\n",
    "\n",
    "We can do this by passing `aug_tfms` (*augmentation transforms*) to `tfms_from_model`, with a list of functions to apply that randomly change the image however we wish. For photos that are largely taken from the side (e.g. most photos of dogs and cats, as opposed to photos taken from the top down, such as satellite imagery) we can use the pre-defined list of functions `transforms_side_on`. We can also specify random zooming of images up to specified scale by adding the `max_zoom` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.precompute=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the other layers have *already* been trained to recognize imagenet photos (whereas our final layers where randomly initialized), so we want to be careful of not destroying the carefully tuned weights that are already there.\n",
    "\n",
    "Generally speaking, the earlier layers (as we've seen) have more general-purpose features. Therefore we would expect them to need less fine-tuning for new datasets. For this reason we will use different learning rates for different layers: the first few layers will be at 1e-4, the middle layers at 1e-3, and our FC layers we'll leave at 1e-2 as before. We refer to this as *differential learning rates*, although there's no standard name for this techique in the literature that we're aware of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=np.array([1e-3,1e-2,1e-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6012216f147e40849e205dab1b369e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=15, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   accuracy                   \n",
      "    0      2.712612   2.083984   0.529044  \n",
      "    1      2.082174   1.605725   0.608235                   \n",
      "    2      1.660177   1.437743   0.653529                   \n",
      "    3      1.733766   1.497985   0.633456                   \n",
      "    4      1.465648   1.342587   0.669265                   \n",
      "    5      1.251007   1.264702   0.690515                   \n",
      "    6      1.138705   1.250694   0.695368                   \n",
      "    7      1.406409   1.529337   0.631176                   \n",
      "    8      1.280752   1.371937   0.66375                    \n",
      "    9      1.149892   1.321249   0.675368                   \n",
      "    10     1.024797   1.263046   0.691618                   \n",
      "    11     0.892343   1.247277   0.702574                    \n",
      "    12     0.789344   1.246163   0.704853                    \n",
      "    13     0.717299   1.236509   0.710441                    \n",
      "    14     0.689813   1.237269   0.708603                    \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([1.23727]), 0.7086029411764706]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr,  4, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another trick we've used here is adding the `cycle_mult` parameter. Take a look at the following chart, and see if you can figure out what the parameter is doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr=np.array([1e-4,1e-3,1e-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.fit(lr, 2, cycle_len=1, cycle_mult=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.sched.plot_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that's what being plotted above is the learning rate of the *final layers*. The learning rates of the earlier layers are fixed at the same multiples of the final layer rates as we initially requested (i.e. the first layers have 100x smaller, and middle layers 10x smaller learning rates, since we set `lr=np.array([1e-4,1e-3,1e-2])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('224_all_200_cyclic_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('224_all_200_cyclic_v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is something else we can do with data augmentation: use it at *inference* time (also known as *test* time). Not surprisingly, this is known as *test time augmentation*, or just *TTA*.\n",
    "\n",
    "TTA simply makes predictions not just on the images in your validation set, but also makes predictions on a number of randomly augmented versions of them too (by default, it uses the original image along with 4 randomly augmented versions). It then takes the average prediction from these images, and uses that. To use TTA on the validation set, we can use the learner's `TTA()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_preds,y = learn.TTA()\n",
    "#probs = np.mean(np.exp(log_preds),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy_np(probs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              \r"
     ]
    }
   ],
   "source": [
    "log_preds_test = learn.TTA(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_preds_test_mean = np.mean(log_preds_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_preds_test2 = learn.predict(is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ -9.16651  -6.95624 -10.2286  ...  -8.25294  -6.14713  -7.73004]\n",
      "  [ -9.14502  -7.95072  -8.65218 ...  -6.69108  -8.61476  -8.25965]\n",
      "  [ -8.4553   -6.60737  -7.68432 ...  -5.47277  -8.24699  -7.40684]\n",
      "  ...\n",
      "  [-10.9798  -11.46378 -11.8281  ... -12.39786  -9.4729  -10.99495]\n",
      "  [ -8.01676  -8.24194  -6.46498 ...  -7.68303  -8.4523   -7.93002]\n",
      "  [ -7.00658  -7.48649  -7.73623 ...  -8.90704  -7.06623  -8.58311]]\n",
      "\n",
      " [[ -9.33689  -7.02735 -10.42415 ...  -8.90454  -6.29876  -7.94663]\n",
      "  [ -8.17896  -7.37478  -7.6134  ...  -6.08325  -8.38057  -7.89706]\n",
      "  [ -7.88029  -6.35378  -7.97466 ...  -5.4484   -8.62132  -7.10884]\n",
      "  ...\n",
      "  [-10.62456 -11.02429 -11.3657  ... -12.19413  -9.1532  -11.00989]\n",
      "  [ -7.8396   -8.37054  -6.02667 ...  -7.5818   -8.09399  -8.12833]\n",
      "  [ -6.44351  -6.90163  -7.52472 ...  -8.22441  -7.15462  -7.91979]]\n",
      "\n",
      " [[ -8.21342  -6.46033  -9.39077 ...  -7.24766  -5.29425  -7.22015]\n",
      "  [ -8.63576  -6.85628  -7.97683 ...  -6.35627  -7.53703  -7.93232]\n",
      "  [ -7.98657  -6.31079  -7.30575 ...  -5.15953  -8.24771  -7.06997]\n",
      "  ...\n",
      "  [-10.69611 -11.85568 -11.49195 ... -12.15416  -9.53378 -11.16745]\n",
      "  [ -7.864    -8.40322  -6.27035 ...  -7.92612  -7.50013  -7.98876]\n",
      "  [ -6.65089  -6.26197  -6.84446 ...  -8.48143  -6.03264  -8.11617]]\n",
      "\n",
      " [[-10.25442  -7.51119 -11.53903 ... -10.43332  -7.79283  -9.22998]\n",
      "  [ -8.67702  -7.9511   -8.18672 ...  -6.81588  -8.9221   -8.40563]\n",
      "  [ -7.23358  -5.76053  -7.34838 ...  -4.9056   -7.74345  -6.21773]\n",
      "  ...\n",
      "  [-10.80518 -11.16388 -11.53477 ... -12.12807  -9.39201 -11.13687]\n",
      "  [ -7.55958  -8.35269  -6.45977 ...  -7.59122  -7.84261  -8.12425]\n",
      "  [ -6.40752  -7.25384  -8.008   ...  -8.64573  -7.29278  -8.23087]]\n",
      "\n",
      " [[ -7.99637  -6.45619  -9.47004 ...  -8.01433  -5.98993  -7.62723]\n",
      "  [ -7.70765  -7.35071  -7.19331 ...  -5.66113  -8.2333   -8.12647]\n",
      "  [ -7.58478  -5.69995  -7.68026 ...  -5.0353   -8.12639  -6.86586]\n",
      "  ...\n",
      "  [-11.4205  -12.15261 -11.93809 ... -12.66424  -9.47812 -11.42408]\n",
      "  [ -8.10637  -8.54565  -6.85462 ...  -7.92589  -8.38249  -7.95853]\n",
      "  [ -6.76371  -6.78935  -7.46374 ...  -8.19368  -6.54406  -8.00254]]]\n"
     ]
    }
   ],
   "source": [
    "print(log_preds_test_mean)\n",
    "\n",
    "#probs_test=np.exp(log_preds_test)\n",
    "\n",
    "#print(probs_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.0001 , 0.00095, 0.00004, ..., 0.00026, 0.00214, 0.00044],\n",
       "        [0.00011, 0.00035, 0.00017, ..., 0.00124, 0.00018, 0.00026],\n",
       "        [0.00021, 0.00135, 0.00046, ..., 0.0042 , 0.00026, 0.00061],\n",
       "        ...,\n",
       "        [0.00002, 0.00001, 0.00001, ..., 0.     , 0.00008, 0.00002],\n",
       "        [0.00033, 0.00026, 0.00156, ..., 0.00046, 0.00021, 0.00036],\n",
       "        [0.00091, 0.00056, 0.00044, ..., 0.00014, 0.00085, 0.00019]],\n",
       "\n",
       "       [[0.00009, 0.00089, 0.00003, ..., 0.00014, 0.00184, 0.00035],\n",
       "        [0.00028, 0.00063, 0.00049, ..., 0.00228, 0.00023, 0.00037],\n",
       "        [0.00038, 0.00174, 0.00034, ..., 0.0043 , 0.00018, 0.00082],\n",
       "        ...,\n",
       "        [0.00002, 0.00002, 0.00001, ..., 0.00001, 0.00011, 0.00002],\n",
       "        [0.00039, 0.00023, 0.00241, ..., 0.00051, 0.00031, 0.0003 ],\n",
       "        [0.00159, 0.00101, 0.00054, ..., 0.00027, 0.00078, 0.00036]],\n",
       "\n",
       "       [[0.00027, 0.00156, 0.00008, ..., 0.00071, 0.00502, 0.00073],\n",
       "        [0.00018, 0.00105, 0.00034, ..., 0.00174, 0.00053, 0.00036],\n",
       "        [0.00034, 0.00182, 0.00067, ..., 0.00574, 0.00026, 0.00085],\n",
       "        ...,\n",
       "        [0.00002, 0.00001, 0.00001, ..., 0.00001, 0.00007, 0.00001],\n",
       "        [0.00038, 0.00022, 0.00189, ..., 0.00036, 0.00055, 0.00034],\n",
       "        [0.00129, 0.00191, 0.00107, ..., 0.00021, 0.0024 , 0.0003 ]],\n",
       "\n",
       "       [[0.00004, 0.00055, 0.00001, ..., 0.00003, 0.00041, 0.0001 ],\n",
       "        [0.00017, 0.00035, 0.00028, ..., 0.0011 , 0.00013, 0.00022],\n",
       "        [0.00072, 0.00315, 0.00064, ..., 0.00741, 0.00043, 0.00199],\n",
       "        ...,\n",
       "        [0.00002, 0.00001, 0.00001, ..., 0.00001, 0.00008, 0.00001],\n",
       "        [0.00052, 0.00024, 0.00157, ..., 0.0005 , 0.00039, 0.0003 ],\n",
       "        [0.00165, 0.00071, 0.00033, ..., 0.00018, 0.00068, 0.00027]],\n",
       "\n",
       "       [[0.00034, 0.00157, 0.00008, ..., 0.00033, 0.0025 , 0.00049],\n",
       "        [0.00045, 0.00064, 0.00075, ..., 0.00348, 0.00027, 0.0003 ],\n",
       "        [0.00051, 0.00335, 0.00046, ..., 0.0065 , 0.0003 , 0.00104],\n",
       "        ...,\n",
       "        [0.00001, 0.00001, 0.00001, ..., 0.     , 0.00008, 0.00001],\n",
       "        [0.0003 , 0.00019, 0.00105, ..., 0.00036, 0.00023, 0.00035],\n",
       "        [0.00115, 0.00113, 0.00057, ..., 0.00028, 0.00144, 0.00033]]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_test=np.exp(log_preds_test_mean)\n",
    "probs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Test output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0001  0.00095 0.00004 ... 0.00026 0.00214 0.00044]\n",
      " [0.00011 0.00035 0.00017 ... 0.00124 0.00018 0.00026]\n",
      " [0.00021 0.00135 0.00046 ... 0.0042  0.00026 0.00061]\n",
      " ...\n",
      " [0.00002 0.00001 0.00001 ... 0.      0.00008 0.00002]\n",
      " [0.00033 0.00026 0.00156 ... 0.00046 0.00021 0.00036]\n",
      " [0.00091 0.00056 0.00044 ... 0.00014 0.00085 0.00019]]\n"
     ]
    }
   ],
   "source": [
    "print(probs_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.argsort(-probs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 99, 138, 253],\n",
       "       [237, 284,  57],\n",
       "       [147,  39, 153],\n",
       "       ...,\n",
       "       [238, 257,  83],\n",
       "       [ 42, 174,  50],\n",
       "       [ 70, 310, 219]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss[0,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_test_top_3 = np.argsort(-probs_test)[0,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112199, 3)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_test_top_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f= open(\"submission_QuickDraw_224_all_200_cyclic_tta_v1.csv\",\"w+\")\n",
    "f.write(\"key_id,word\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.test_ds.fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ids = [ x.split('/')[1].split('.png')[0] for x in data.test_ds.fnames] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9483104542098626'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for i in range(prob_test_top_3.shape[0]):\n",
    "    #print(i)\n",
    "    f.write(key_ids[i]+\",\")\n",
    "    for j in range(prob_test_top_3.shape[1]):\n",
    "        #print(j)\n",
    "        #print(prob_test_top_3[i][j])\n",
    "        #print(data.classes[prob_test_top_3[i][j]])\n",
    "        \n",
    "        f.write(data.classes[prob_test_top_3[i][j]]+ \" \")\n",
    "        #print(data.classes[j])\n",
    "    f.write(\"\\n\")\n",
    "#labels\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission_QuickDraw_224_all_200_cyclic_tta_v1.csv' target='_blank'>submission_QuickDraw_224_all_200_cyclic_tta_v1.csv</a><br>"
      ],
      "text/plain": [
       "/home/mayjain/fastai/courses/dl1/submission_QuickDraw_224_all_200_cyclic_tta_v1.csv"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('submission_QuickDraw_224_all_200_cyclic_tta_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels_probs_test_top_3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
